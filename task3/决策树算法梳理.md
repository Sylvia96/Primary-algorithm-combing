## 一、信息论基础（熵 联合熵 条件熵 信息增益 基尼不纯度） 
### 1、熵  
熵是一种不确定性的度量，在自然语言处理或者机器学习领域中一般指代香农熵（Shannon entropy）。  
### 2、联合熵  
联合熵就是度量一个联合分布的随机系统的不确定度  
### 3、条件熵  
定义为X给定条件下，Y的条件概率分布的熵对X的数学期望  
### 4、信息增益  
信息增益（Kullback–Leibler divergence）又称information divergence，information gain，relative entropy 或者KLIC。  
在概率论和信息论中，信息增益是非对称的，用以度量两种概率分布P和Q的差异。信息增益描述了当使用Q进行编码时，再使用P进行编码的差异。通常P代表样本或观察值的分布，也有可能是精确计算的理论分布。Q代表一种理论，模型，描述或者对P的近似。  
### 5、基尼不纯度  
将来自集合中的某种结果随机应用于集合中某一数据项的预期误差率。  
  
## 二、回归树原理  
回归决策树用于处理输出为连续型的数据。回归决策树在选取划分点，就希望划分的两个分支的误差越小越好。  

其余内容待整理…
